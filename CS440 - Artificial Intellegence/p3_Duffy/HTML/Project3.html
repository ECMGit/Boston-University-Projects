<html><head><title>Copy of CS440 Project 3 or: How I Learned to Stop Worrying and Love Your Mom</title><style type="text/css">ol{margin:0;padding:0}p{margin:0}.c7{text-indent:36pt;margin-left:144pt}.c8{color:inherit;text-decoration:inherit}.c5{text-indent:36pt;margin-left:108pt}.c10{max-width:468pt;padding:72pt 72pt 72pt 72pt}.c4{color:#1155cc;text-decoration:underline}.c2{text-align:center}.c3{font-weight:bold}.c11{color:#f3f3f3}.c9{font-size:18pt}.c1{height:11pt}.c12{margin-left:144pt}.c6{background-color:#ffffff}.c0{direction:ltr}.title{padding-top:24pt;line-height:1.15;text-align:left;color:#000000;font-size:36pt;font-family:Arial;font-weight:bold;padding-bottom:6pt}.subtitle{padding-top:18pt;line-height:1.15;text-align:left;color:#666666;font-style:italic;font-size:24pt;font-family:Georgia;padding-bottom:4pt}body{color:#000000;font-size:11pt;font-family:Arial}h1{padding-top:24pt;line-height:1.15;text-align:left;color:#000000;font-size:18pt;font-family:Arial;font-weight:bold;padding-bottom:6pt}h2{padding-top:18pt;line-height:1.15;text-align:left;color:#000000;font-size:14pt;font-family:Arial;font-weight:bold;padding-bottom:4pt}h3{padding-top:14pt;line-height:1.15;text-align:left;color:#666666;font-size:12pt;font-family:Arial;font-weight:bold;padding-bottom:4pt}h4{padding-top:12pt;line-height:1.15;text-align:left;color:#666666;font-style:italic;font-size:11pt;font-family:Arial;padding-bottom:2pt}h5{padding-top:11pt;line-height:1.15;text-align:left;color:#666666;font-size:10pt;font-family:Arial;font-weight:bold;padding-bottom:2pt}h6{padding-top:10pt;line-height:1.15;text-align:left;color:#666666;font-style:italic;font-size:10pt;font-family:Arial;padding-bottom:2pt}</style></head><body class="c6 c10"><p class="c2 c0"><span class="c9">Project 3: Speech Recognition</span></p><p class="c0"><span>Group members:</span></p><p class="c0"><span>Adam Even &ldquo;Bagels and Booze&rdquo; Engel</span></p><p class="c0"><span>Tim &ldquo;Timothy-Tim-Ta-Roo&rdquo; Duffy</span></p><p class="c0"><span>Chris &ldquo;Died for Tim&rsquo;s Shins&rdquo; Hall</span></p><hr><p class="c1 c0"><span></span></p><p class="c1 c0"><span></span></p><p class="c0"><span class="c9">Purpose:</span></p><p class="c0"><span>Develop a Hidden Markov Model system capable of parsing a basic English sentence and comprehending its validity.</span></p><p class="c1 c0"><span></span></p><hr><p class="c1 c0"><span></span></p><p class="c1 c0"><span></span></p><p class="c0"><span class="c9">Implementation:</span></p><p class="c0"><span>We utilized the Java programming language for this project using the Eclipse IDE. The final source code was developed by Tim Duffy in partnership with Adam Even Engel and Chris Hall. The program implements the classic Veterbi and Baum-Welch algorithms as well as the forward-backward algorithm.</span></p><p class="c1 c0"><span></span></p><hr><p class="c1 c0"><span></span></p><p class="c1 c0"><span></span></p><p class="c0"><span class="c9">Development:</span></p><p class="c1 c0"><span></span></p><p class="c0"><span>To solve this problem, we identified three main point of functionality to implement:</span></p><p class="c1 c0"><span></span></p><p class="c0"><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c3">1. Pattern Recognition</span><span>: For each observation set, apply the &lsquo;forward&rsquo; algorithm to</span></p><p class="c0 c5"><span>&nbsp; &nbsp; &nbsp; the set, and report the probability of the sequence</span></p><p class="c5 c1 c0"><span></span></p><p class="c0"><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c3">2. State Path Determination: </span><span>For each observation set, apply the Viterbi algorithm</span></p><p class="c0 c7"><span>&nbsp; &nbsp;to determine the optimal state path and its probability</span></p><p class="c1 c0"><span></span></p><p class="c0"><span class="c3">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3. Model Optimization: </span><span>For each observation set, apply the Baum-Welch algorithm to</span></p><p class="c12 c0"><span>&nbsp; &nbsp; &nbsp;produce an HMM which is optimized to produce the maximum &nbsp; </span></p><p class="c0 c12"><span>&nbsp; &nbsp; &nbsp;likelihood of the given sequence</span></p><p class="c1 c0"><span></span></p><p class="c0"><span>Development of this project started with parsing in the HMM file given by the user. We then created a State variable, which is a custom made class. Each instance of a &lsquo;State&rsquo; class holds its name, its initial (pi) probability, the probabilities of transitioning from the state to each of the other states, and the probability of observing each vocabulary word while in that state. Using this representation of the HMM, we were able to implement the necessary algorithms quite easily.</span></p><p class="c1 c0"><span></span></p><p class="c0"><span>For state recognition, we implemented the &lsquo;forward&rsquo; part of the forward-backward process to calculate P(O | &#8516;), or the probability of observing the sequence O, given a certain HMM. The calculation of forward probabilities is:</span></p><p class="c1 c0"><span></span></p><p class="c1 c0"><span></span></p><p class="c0 c2"><img height="279" src="images/image00.png" width="445"></p><p class="c1 c0"><span></span></p><p class="c2 c0"><span>To determine the path of states that results in the highest observation probability, we used the Veterbi algorithm. The calculation of the optimal path (and its probability) is as follows:</span></p><p class="c2 c1 c0"><span></span></p><p class="c2 c0"><img height="451" src="images/image01.png" width="392"></p><p class="c1 c0"><span></span></p><p class="c0"><span>Finally, we used the Baum-Welch algorithm to create an HMM which maximizes the probability of observing a given observation set. We calculated new transition and observation probabilities for each state based on this formula. For more information, click here: </span><span class="c4"><a class="c8" href="http://www.cs.bu.edu/fac/betke/cs440/restricted/papers/rabiner.pdf">http://www.cs.bu.edu/fac/betke/cs440/restricted/papers/rabiner.pdf</a></span></p><hr><p class="c1 c0"><span></span></p><p class="c1 c0"><span></span></p><p class="c0"><span class="c9">Questions:</span></p><p class="c1 c0"><span class="c9"></span></p><p class="c2 c0"><span class="c3">For the current application, why does this probability seem lower than we expect? What does this probability tell you? Does the current HMM always give a reasonable answer? For instance, what is the output probability for the below sentence?</span></p><p class="c1 c0"><span class="c3"></span></p><p class="c0"><span>The probability seems lower because two of the three are actually valid sentences. The calculated probabilities represent the likelihood that a given string of words is a valid sentence. &nbsp;&ldquo;Kids play chess&rdquo; and &ldquo;Robots eat food&rdquo; could both be observed in real-world English usage, so their observation probability should be much higher. Having such a low probability suggests multiple things. It may not look at the subject as the initial state, or we may simply be working with a poorly-designed model. The example sentences yield the following output:</span></p><p class="c1 c0"><span></span></p><p class="c0"><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Robots do kids play chess: 0.001512</span></p><p class="c0"><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Chess eat play kids: 0.0</span></p><p class="c1 c0"><span></span></p><p class="c0"><span>The first string is actually a valid sentence - &ldquo;Robots, do kids play chess?&rdquo; However, it gives an extremely low probability, indicating that the HMM is not particularly strong (or creative). The second sentence rightfully has a probability of 0 because it&rsquo;s just a collection of words that hold no meaning.</span></p><p class="c1 c0"><span></span></p><p class="c2 c0"><span class="c3">What can we tell from the reported optimal path for syntax analysis purpose? Can the HMM always correctly distinguish &quot;statement&quot; from &quot;question&quot; sentence? Why?</span></p><p class="c1 c0"><span class="c3"></span></p><p class="c0"><span>The best thing we can tell from the syntax analysis is what the model assumes the string says. From analyzing the supplied files, it does a fairly good job picking up which word is associated with which part of speech. Using the previous question&rsquo;s first example sentence, however, yields a very low probability for one specific reason: the model cannot properly recognize questions. Statements and questions have different structures. What works for one does not work for the other. Therefore, the HMM reads &ldquo;Robots do kids play chess&rdquo; as a nonsensical sentence rather than &ldquo;Robots, do kids play chess?&rdquo; as a valid question. However, &ldquo;Robots, do kids want to play chess in my butthole?&rdquo; is also a valid question, given a larger vocabulary.</span></p><p class="c1 c0"><span></span></p><p class="c0 c1"><span></span></p><p class="c2 c0"><span class="c3">Why should you not try to optimize an HMM with zero observation probability? </span></p><p class="c1 c0"><span></span></p><p class="c0"><span>When the HMM has a zero observation probability, optimization is moot. Due the the zero observation probability, the system does not know enough to optimize it. If an optimization occurred and changed it to a non-zero probability, unfounded and unsupported results could occur. </span></p><p class="c1 c0"><span></span></p><p class="c2 c0"><span class="c3">What kinds of changes will you need to make in the above HMM?</span></p><p class="c1 c0"><span></span></p><p class="c0"><span>If we wanted to add new states such as &ldquo;PRESENT TENSE&rdquo; or &ldquo;ADVERB&rdquo;, we would have to expand the A matrix to show the transition probabilities to and from these new states. We would also have to add to the B matrix, the probability of observing each of the vocabulary words while in these new states. Finally, we would need to expand the pi matrix to include the probabilities of beginning in each state.</span></p><p class="c1 c0"><span></span></p><p class="c0"><span class="c6">In order to add 2 more syntax structures, would require an increase of 2 rows and 2 columns on the a and b matrices, and two values added to the pi array. An example of the updated a,b, and pi may look as follows:</span></p><p class="c1 c0"><span></span></p><p class="c0"><span>a:<br>0.0 0.4 0.6 0.0 0.0 0.0<br>0.7 0.0 0.3 0.0 0.0 0.0<br>0.0 0.0 0.0 0.4 0.5 0.1<br>0.0 0.0 0.0 0.6 0.3 0.1</span></p><p class="c0"><span>0.3 0.0 0.7 0.0 0.0 0.0</span></p><p class="c0"><span>0.0 0.0 0.0 0.2 0.8 0.0</span></p><p class="c0"><span><br>b:<br>0.5 0.4 0.0 0.0 0.0 0.0 0.05 0.05<br>0.0 0.0 0.5 0.5 0.0 0.0 0.0 0.0<br>0.0 0.0 0.0 0.0 0.5 0.5 0.0 0.0<br>0.1 0.2 0.0 0.0 0.0 0.0 0.3 0.4</span></p><p class="c0"><span>0.0 0.0 0.0 0.1 0.2 0.0 0.7 0.0</span></p><p class="c0"><span>0.0 0.0 0.0 0.0 0.6 0.2 0.0 0.2</span></p><p class="c0"><span><br>pi:<br>0.3 0.3 0.1 0.0 0.2 0.1</span></p><p class="c1 c0"><span></span></p><hr><p class="c1 c0"><span></span></p><p class="c1 c0"><span></span></p><p class="c0"><span class="c9">Conclusion:</span></p><p class="c0"><span>In this project, we implemented a very simple natural language processor capable of determining the validity of a string of words in English. It was limited to a fairly short corpus of words. Though it could ably parse the sentences, it could not recognize the difference in syntax between a statement and a question. In short, the hidden Markov model we built was quite limited in scope. To expand, we would need to refine the algorithm to recognize different structures and add states. We would also want to create a larger corpus and maybe allow the program to add to the corpus itself by interpolating part of speech from surrounding words.</span></p></body></html>